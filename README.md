# Effect of LLM quantization on inference energy consumption

This repo is WiP. The idea is to measure and compare the energy consumption
from inference of an LLM in quantized and unquantized versions. 

Datasets:
- SQuAD v2
- CNN-Daily Mail

Sample 50 questions from each 10 times?

Check out this for inspiration: https://github.com/sashavor/co2_inference/tree/main from the paper https://arxiv.org/abs/2311.16863 